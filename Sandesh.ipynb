{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-351-4dcd207083af>, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-351-4dcd207083af>\"\u001b[1;36m, line \u001b[1;32m21\u001b[0m\n\u001b[1;33m    from scipy.stats.stats import pearsonr\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "matplotlib.rcParams['axes.labelsize'] = 14\n",
    "matplotlib.rcParams['xtick.labelsize'] = 12\n",
    "matplotlib.rcParams['ytick.labelsize'] = 12\n",
    "matplotlib.rcParams['text.color'] = 'k'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.utils.testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "\n",
    "\n",
    "## Configurable parameters\n",
    "\n",
    "# Make True to see plots while finding related features\n",
    "SHOW_PLOTS = False\n",
    "\n",
    "# Make True to see feature importance graphs\n",
    "SHOW_FEATURE_IMPORTANCE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file():\n",
    "    '''\n",
    "    Reads the challenge data 'Sandesh' and preprocess it:\n",
    "        -tranpose the raw data\n",
    "        -set datetime as index \n",
    "        -set 'Generic_LookupKeys' as columns\n",
    "        -remove descriptive rows\n",
    "        \n",
    "    Input: NA.\n",
    "    Output: full_data, target_data(with only target variables)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    data_df = pd.read_excel('data/ChallengeData-Sandesh.xlsx')\n",
    "    \n",
    "    #save the dates for indexing\n",
    "    date_index = data_df.iloc[1,9:].dt.strftime('%Y/%m')\n",
    "    \n",
    "    #find the interested variables\n",
    "    target_rows = [25, 26, 76, 77, 79, 80, 82, 83, 85, 86, 91, 92]\n",
    "    \n",
    "    target_data = data_df.iloc[[x-2 for x in target_rows], :]\n",
    "    Target_Generic_LookupKeys = list(target_data.iloc[:,7])\n",
    "    \n",
    "    #dropping the descriptive rows and reset index and columns\n",
    "    target_data = target_data.T.iloc[9:,:]\n",
    "    target_data.set_index(date_index, inplace=True)\n",
    "    target_data.columns = Target_Generic_LookupKeys\n",
    "    target_data.dropna(how='all', inplace=True)\n",
    "    \n",
    "    #dataframe with all the feature lookupkeys\n",
    "    Generic_LookupKeys = list(data_df.iloc[2:,7])\n",
    "    full_data = data_df.iloc[2:,:].T.iloc[9:,:]\n",
    "    full_data.set_index(date_index, inplace=True)\n",
    "    full_data.columns = Generic_LookupKeys\n",
    "    full_data.dropna(how='all', inplace=True)\n",
    "    \n",
    "    return full_data, target_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_correlated_features(full_data, target_data):\n",
    "    '''\n",
    "    Checks correlation between every pair of features and keeps a list of highest correlated features for every feature.\n",
    "    These correlated features are used in modelling.\n",
    "    Input: submission and train dataframes.\n",
    "    Output: A dictionary of correlated features for every feature.\n",
    "    '''\n",
    "    \n",
    "    related_features = {}\n",
    "    related_features_names = []\n",
    "    related_features_correlations = []\n",
    "    \n",
    "    Target_Generic_LookupKeys = target_data.columns.tolist()\n",
    "    Generic_LookupKeys = full_data.columns.tolist()\n",
    "    \n",
    "    \n",
    "    # For every lookupkey in target_data, find the correlated features\n",
    "    for generic_lookupkey in Target_Generic_LookupKeys:\n",
    "        #print(generic_lookupkey)\n",
    "        \n",
    "        # Initialize the related features list for this feature\n",
    "        related_features[generic_lookupkey] = []\n",
    "        generic_lookups = Generic_LookupKeys.copy()\n",
    "        \n",
    "        # Remove the currenyt feature itself from all the other features\n",
    "        if generic_lookupkey in generic_lookups:\n",
    "            generic_lookups.remove(generic_lookupkey)\n",
    "        \n",
    "        list_series = []\n",
    "        list_series.append(full_data[generic_lookupkey].dropna().tolist())\n",
    "    \n",
    "        # For all the other features, find correlation coefficient with the current feature\n",
    "        for generic_lookup in generic_lookups:\n",
    "            list_series.append(full_data[generic_lookup].dropna().tolist())\n",
    "            if SHOW_PLOTS and len(list_series[0]) > 0 and len(list_series[-1]) > 0:\n",
    "                initial_analysis_plots([list_series[0], list_series[-1]], [generic_lookupkey, generic_lookup])\n",
    "            min_len = min(len(list_series[0]), len(list_series[-1]))\n",
    "            if min_len > 1:\n",
    "                p_coeff = pearsonr(list_series[0][-min_len:], list_series[-1][-min_len:])[0]\n",
    "                # print(p_coeff)\n",
    "                # If correlation coefficient is more than 0.8 or less than -0.8, include this feature\n",
    "                if p_coeff > 0.8 or p_coeff < -0.8:\n",
    "                    related_features[generic_lookupkey].append(generic_lookup)\n",
    "                    related_features[generic_lookupkey].append(generic_lookup)\n",
    "                    related_features_names.append(generic_lookup)\n",
    "                    related_features_correlations.append(p_coeff)\n",
    "\n",
    "                \n",
    "        related_features_df = pd.DataFrame()\n",
    "        related_features_df['features'] = related_features_names\n",
    "        related_features_df['correlation'] = np.abs(related_features_correlations)\n",
    "        \n",
    "        if SHOW_FEATURE_IMPORTANCE and related_features_df.shape[0] > 0:\n",
    "            ax = related_features_df.plot(x='features', y='correlation', kind='bar', title=generic_lookupkey, figsize=(12, 8), legend=True, fontsize=12)\n",
    "            ax.set_xlabel('feature name', fontsize=12)\n",
    "            ax.set_ylabel('correlation value', fontsize=12)\n",
    "            plt.show()\n",
    "                    \n",
    "    return related_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_related_features(df, train, related_features, generic_lookup):\n",
    "    '''\n",
    "    Joins all the correlated features to the current feature (generic lookup). Since, length of data could be different \n",
    "    for all the features so overall length of dataframe is also tracked.\n",
    "    Input:  df- A dataframe having only current generic_lookup key data as single column.\n",
    "            train- train dataframe.\n",
    "            related_features- dictionary of related features for every lookupkey.\n",
    "            generic_lookup- the generic lookup in consideration.\n",
    "    Output: A dataframe having all the correlated features to the current generic lookup.\n",
    "    '''\n",
    "    smallest_series_length = df.shape[0]\n",
    "    max_other_series_name = 0\n",
    "    max_other_series_length = -1\n",
    "    \n",
    "    #print('number of related series: ', len(related_features[generic_lookup]))\n",
    "    for related_feature in related_features[generic_lookup]:\n",
    "        #print(related_feature)\n",
    "        series_data = full_data[related_feature].tolist()\n",
    "        #print('length of series data: ', len(series_data))\n",
    "        if len(series_data) >= smallest_series_length:\n",
    "            df[related_feature] = series_data[-smallest_series_length:]\n",
    "        elif len(series_data) > max_other_series_length:\n",
    "            max_other_series_name = related_feature\n",
    "            max_other_series_length = len(series_data)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "HELPER FUNCTIONS\n",
    "'''\n",
    "\n",
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "#Quantitative Scoring using MAPE\n",
    "def MAPE(gt, pred):\n",
    "    mape = []\n",
    "\n",
    "    for g, p in zip(gt, pred):\n",
    "        mape.append(max(0, 1 - abs((g-p)/g)))\n",
    "\n",
    "    return np.mean(mape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def best_parameters_sarimax(series):\n",
    "    '''\n",
    "    Finds the best parameters for a given series for SARIMAX algorithm.\n",
    "    Input: series: the series for which the parameters are to be determined.\n",
    "    Output: the best parameters for the series and model.\n",
    "    '''\n",
    "    result_param = -1\n",
    "    result_param_seasonal = -1\n",
    "    p = d = q = range(0, 2)\n",
    "    pdq = list(itertools.product(p, d, q))\n",
    "    seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n",
    "    for param in pdq:\n",
    "        for param_seasonal in seasonal_pdq:\n",
    "            try:\n",
    "                mod = sm.tsa.statespace.SARIMAX(series,\n",
    "                                                order=param,\n",
    "                                                seasonal_order=param_seasonal,\n",
    "                                                enforce_stationarity=False,\n",
    "                                                enforce_invertibility=False)\n",
    "                results = mod.fit()\n",
    "                #print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n",
    "                try:\n",
    "                    if results.aic < minimum:\n",
    "                        result_param = param\n",
    "                        result_param_seasonal = param_seasonal\n",
    "                except:\n",
    "                    result_param = param\n",
    "                    result_param_seasonal = param_seasonal\n",
    "                    minimum = results.aic\n",
    "            except:\n",
    "                continue\n",
    "    return result_param, result_param_seasonal\n",
    "\n",
    "def apply_model_sarimax(series, best_param, best_param_seasonal):\n",
    "    '''\n",
    "    Makes and trains SARIMAX model on the given series and parameters.\n",
    "    Input:  series: series on which the model is to be trained.\n",
    "            best_param, best_param_seasonal: best parameters for modelling\n",
    "    Output: Trained model\n",
    "    '''\n",
    "    mod = sm.tsa.statespace.SARIMAX(series,\n",
    "                                    order=best_param,\n",
    "                                    seasonal_order=best_param_seasonal,\n",
    "                                    enforce_stationarity=False,\n",
    "                                    enforce_invertibility=False)\n",
    "    results = mod.fit()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_varmax(train_df, generic_lookup, validation_values):\n",
    "    '''\n",
    "    Train a VARMAX and predicts on the validation data.\n",
    "    Input:  train_df- training dataframe.\n",
    "            generic_lookup- the generic_llokup for which we are going to predict.\n",
    "            validation_values- the validation set to compare with.\n",
    "    Output: The validation set RMSE value.\n",
    "    '''\n",
    "    try:\n",
    "        model = VARMAX(train_df[:-5], order=(2, 0), trend='c', enforce_stationarity=False, enforce_invertibility=False)\n",
    "        model_result = model.fit(maxiter=1000, disp=False)\n",
    "        validation_forecast = model_result.forecast(steps=5)[generic_lookup].tolist()\n",
    "        mape_varmax = MAPE(validation_values, validation_forecast)\n",
    "        return mape_varmax\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_sarimax(train_df, generic_lookup, validation_values):\n",
    "    '''\n",
    "    Train a SARIMAX and predicts on the validation data.\n",
    "    Input:  train_df- training dataframe.\n",
    "            generic_lookup- the generic_llokup for which we are going to predict.\n",
    "            validation_values- the validation set to compare with.\n",
    "    Output: The validation set RMSE value.\n",
    "    '''\n",
    "    try:\n",
    "        best_param, best_param_seasonal = best_parameters_sarimax(train_df[generic_lookup][:-5])\n",
    "        results = apply_model_sarimax(train_df[generic_lookup][:-5], best_param, best_param_seasonal)\n",
    "        forecast = results.get_forecast(steps=5)\n",
    "        validation_forecast = forecast.predicted_mean.tolist() # What is predicted_mean????\n",
    "        \n",
    "        #Computing validation metrics MAPE\n",
    "        mape_sarimax = MAPE(validation_values, validation_forecast)\n",
    "        return mape_sarimax\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ignore_warnings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-352-5f1764926cec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m@\u001b[0m\u001b[0mignore_warnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mConvergenceWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcross_validation_and_model_comparison\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_steps_to_predict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgeneric_lookup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     '''\n\u001b[0;32m      4\u001b[0m     \u001b[0mCompares\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mthree\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;31m'\u001b[0m \u001b[0mvalidation\u001b[0m \u001b[0mMAPE\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbest\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mInput\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[0mtrain_df\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdataframe\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mgeneric\u001b[0m \u001b[0mlookup\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtraining\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ignore_warnings' is not defined"
     ]
    }
   ],
   "source": [
    "@ignore_warnings(category=ConvergenceWarning)\n",
    "def cross_validation_and_model_comparison(train_df, number_of_steps_to_predict, generic_lookup, validation_values):\n",
    "    '''\n",
    "    Compares the three models' validation MAPE values and returns the predictions for the best model.\n",
    "    Input:  train_df- the dataframe of current generic lookup for training and prediction.\n",
    "            train- full training dataframe.\n",
    "            number_of_steps_to_predict- number of future steps to predict.\n",
    "            validation_values- the validation set for comparison.\n",
    "    Output: The final forecast for the current generic lookup.\n",
    "    '''\n",
    "\n",
    "    # Check validation MAPE for all three the models\n",
    "    validation_mape_varmax = train_and_predict_varmax(train_df, generic_lookup, validation_values)\n",
    "    validation_mape_sarimax = train_and_predict_sarimax(train_df, generic_lookup, validation_values)\n",
    "    #validation_mape_LSTM = train_and_predict_LSTM()\n",
    "    \n",
    "    if validation_mape_varmax != 1 and validation_mape_sarimax != 1:\n",
    "        print('validation_mape_varmax:', validation_mape_varmax)\n",
    "        print('validation_mape_sarimax:', validation_mape_sarimax)\n",
    "        #print('validation_mape_LSTM:', validation_mape_LSTM)\n",
    "\n",
    "    # If validation RMSE of VARMAX is lesser than SARIMAX, use VARMAX else SARIMAX\n",
    "    if validation_mape_varmax != 1 and validation_mape_varmax < validation_mape_sarimax:\n",
    "        try:\n",
    "            model = VARMAX(train_df, order=(2, 0), trend='c', enforce_stationarity=False, enforce_invertibility=False)\n",
    "            model_result = model.fit(maxiter=1000, disp=False)\n",
    "            forecast = model_result.forecast(steps=number_of_steps_to_predict)\n",
    "            forecast = forecast[generic_lookup].tolist()\n",
    "            return forecast\n",
    "        except:\n",
    "            best_param, best_param_seasonal = best_parameters_sarimax(train_df[generic_lookup])\n",
    "            results = apply_model_sarimax(train_df[generic_lookup], best_param, best_param_seasonal)\n",
    "        #   print(results.summary().tables[1])\n",
    "        #   plt.show()\n",
    "            forecast = results.get_forecast(steps=number_of_steps_to_predict)\n",
    "            pred_ci = forecast.conf_int()\n",
    "            forecast = forecast.predicted_mean.tolist()\n",
    "            return forecast\n",
    "    \n",
    "    elif validation_mape_sarimax != 1:\n",
    "        best_param, best_param_seasonal = best_parameters_sarimax(train_df[generic_lookup])\n",
    "        results = apply_model_sarimax(train_df[generic_lookup], best_param, best_param_seasonal)\n",
    "    #   print(results.summary().tables[1])\n",
    "    #   plt.show()\n",
    "        forecast = results.get_forecast(steps=number_of_steps_to_predict)\n",
    "        pred_ci = forecast.conf_int()\n",
    "        forecast = forecast.predicted_mean.tolist()\n",
    "        return forecast\n",
    "        \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_number_of_steps_to_predict(submission, generic_lookup):\n",
    "#     '''\n",
    "#     Returns the number of future data points for which the predictions are to be made.\n",
    "#     Input:  submission: the submission file\n",
    "#             generic_lookup: the generic llokup for which the predictions are to be made\n",
    "#     Output: the number of future time steps for which the predictions are to be made.\n",
    "#     '''\n",
    "#     return submission[submission['Generic LookupKey'] == generic_lookup].shape[0]\n",
    "##################\n",
    "# REPLACING\n",
    "\n",
    "\n",
    "\n",
    "def make_predictions(target_data, full_data, related_features):\n",
    "    '''\n",
    "    The parent function to join correlated features, get number of steps, does cross validation and compares models.\n",
    "    Input:  submission: submission dataframe.\n",
    "            train: train dataframe.\n",
    "            related_features: dictionary of related features.\n",
    "    Output: Full forecast for all the generic lookup.\n",
    "    '''\n",
    "    # Initialize a list for all generic lookup.\n",
    "    full_forecast = {}\n",
    "    \n",
    "    \n",
    "    # For every generic lookup in submission.\n",
    "    for generic_lookup in target_data.columns:\n",
    "        \n",
    "        print('generic lookup: ', generic_lookup)\n",
    "        train_df = pd.DataFrame()\n",
    "        series_data = target_data[generic_lookup].dropna().tolist()\n",
    "        train_df[generic_lookup] = series_data\n",
    "\n",
    "        # Join all correlated features to dataframe of this generic lookup\n",
    "        train_df = join_related_features(train_df, full_data, related_features, generic_lookup)\n",
    "        #mostly only half the number of feature are kept\n",
    "        #print(\"number of related usable features: {}\".format(train_df.shape[0]))\n",
    "  \n",
    "\n",
    "        # Get number of steps to predict for this generic lookup\n",
    "        number_of_steps_to_predict = 12\n",
    "\n",
    "        # Take last 5 as validation\n",
    "        validation_values = train_df[generic_lookup][-5:]\n",
    "    \n",
    "    \n",
    "        # Check cross validation and do final prediction.\n",
    "        forecast = cross_validation_and_model_comparison(train_df, number_of_steps_to_predict, generic_lookup, validation_values)\n",
    "        if forecast == 1:\n",
    "            print(\"TERRIBLEEEEEEEEEEEEEEEE\")\n",
    "            #will mean work better??????\n",
    "            forecast = [train_df[generic_lookup].mean()]*number_of_steps_to_predict\n",
    "        full_forecast[generic_lookup] = forecast\n",
    "    \n",
    "    return full_forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files\n",
      "Finding related features\n",
      "Segment 1 - Sandesh Brand 2Sandesh Brand 2BroadbandTortoiseRevenue - Tortoise\n",
      "Segment 1 - Sandesh Brand 2Sandesh Brand 2BroadbandRabbitRevenue - Rabbit\n",
      "Segment 1 - Sandesh Brand 2Sandesh Brand 2BroadbandTortoiseAverage Revenue per existing customer Excl Line Rental - Tortoise\n",
      "Segment 1 - Sandesh Brand 2Sandesh Brand 2BroadbandRabbitAverage Revenue per existing customer Excl Line Rental - Rabbit\n",
      "Segment 1 - Sandesh Brand 2Sandesh Brand 2BroadbandTortoiseLeavers - Tortoise\n",
      "Segment 1 - Sandesh Brand 2Sandesh Brand 2BroadbandRabbitLeavers - Rabbit \n",
      "Segment 1 - Sandesh Brand 2Sandesh Brand 2BroadbandTortoiseClosing Base - Tortoise\n",
      "Segment 1 - Sandesh Brand 2Sandesh Brand 2BroadbandRabbitClosing Base - Rabbit \n",
      "Segment 1 - Sandesh Brand 2Sandesh Brand 2BroadbandTortoiseGross Adds - Tortoise\n",
      "Segment 1 - Sandesh Brand 2Sandesh Brand 2BroadbandRabbitGross Adds - Rabbit \n",
      "Segment 1 - Sandesh Brand 2Sandesh Brand 2BroadbandTortoiseNet Migrations - Tortoise\n",
      "Segment 1 - Sandesh Brand 2Sandesh Brand 2BroadbandRabbitNet Migrations - Rabbit \n",
      "Making full forecast\n",
      "generic lookup:  Segment 1 - Sandesh Brand 2Sandesh Brand 2BroadbandTortoiseRevenue - Tortoise\n",
      "validation_mape_varmax: -1\n",
      "validation_mape_sarimax: 0.9227985014140859\n",
      "generic lookup:  Segment 1 - Sandesh Brand 2Sandesh Brand 2BroadbandRabbitRevenue - Rabbit\n",
      "validation_mape_varmax: -1\n",
      "validation_mape_sarimax: 0.9239137002141014\n",
      "generic lookup:  Segment 1 - Sandesh Brand 2Sandesh Brand 2BroadbandTortoiseAverage Revenue per existing customer Excl Line Rental - Tortoise\n",
      "validation_mape_varmax: -1\n",
      "validation_mape_sarimax: 0.5872194428806548\n",
      "generic lookup:  Segment 1 - Sandesh Brand 2Sandesh Brand 2BroadbandRabbitAverage Revenue per existing customer Excl Line Rental - Rabbit\n",
      "validation_mape_varmax: -1\n",
      "validation_mape_sarimax: 0.1728959678189963\n",
      "generic lookup:  Segment 1 - Sandesh Brand 2Sandesh Brand 2BroadbandTortoiseLeavers - Tortoise\n",
      "validation_mape_varmax: -1\n",
      "validation_mape_sarimax: 0.5677793599263046\n",
      "generic lookup:  Segment 1 - Sandesh Brand 2Sandesh Brand 2BroadbandRabbitLeavers - Rabbit \n",
      "validation_mape_varmax: -1\n",
      "validation_mape_sarimax: 0.5539002834033647\n",
      "generic lookup:  Segment 1 - Sandesh Brand 2Sandesh Brand 2BroadbandTortoiseClosing Base - Tortoise\n",
      "validation_mape_varmax: -1\n",
      "validation_mape_sarimax: 0.9558817927566485\n",
      "generic lookup:  Segment 1 - Sandesh Brand 2Sandesh Brand 2BroadbandRabbitClosing Base - Rabbit \n",
      "validation_mape_varmax: -1\n",
      "validation_mape_sarimax: 0.9955860385755667\n",
      "generic lookup:  Segment 1 - Sandesh Brand 2Sandesh Brand 2BroadbandTortoiseGross Adds - Tortoise\n",
      "validation_mape_varmax: -1\n",
      "validation_mape_sarimax: 0.7600773515221568\n",
      "generic lookup:  Segment 1 - Sandesh Brand 2Sandesh Brand 2BroadbandRabbitGross Adds - Rabbit \n",
      "validation_mape_varmax: -1\n",
      "validation_mape_sarimax: 0.5210751565004419\n",
      "generic lookup:  Segment 1 - Sandesh Brand 2Sandesh Brand 2BroadbandTortoiseNet Migrations - Tortoise\n",
      "validation_mape_varmax: -1\n",
      "validation_mape_sarimax: 0.04214960722513923\n",
      "generic lookup:  Segment 1 - Sandesh Brand 2Sandesh Brand 2BroadbandRabbitNet Migrations - Rabbit \n",
      "validation_mape_varmax: -1\n",
      "validation_mape_sarimax: 0.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    print('Reading files')\n",
    "    full_data, target_data = read_file()\n",
    "    print('Finding related features')\n",
    "    related_features = find_correlated_features(full_data, target_data)\n",
    "    print('Making full forecast')\n",
    "    full = make_predictions(target_data, full_data, related_features)\n",
    "#     submission['Value'] = full_forecasts\n",
    "#     submission.to_csv('../output/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = [2.5, 3.2, 4.7]\n",
    "pred = [3, 3.5, 5.2]\n",
    "\n",
    "mape = []\n",
    "\n",
    "for g, p in zip(gt, pred):\n",
    "    mape.append(max(0, 1 - abs(g-p)/g))\n",
    "\n",
    "return np.mean(mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8, 0.90625, 0.8936170212765957]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8666223404255319"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8666223404255319"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAPE(gt, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extend datetime data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = datetime.strptime('2010-8', '%Y-%m').date()\n",
    "time1 = datetime.strptime('2010-9', '%Y-%m').date()\n",
    "diff = time1 - time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'datetime.date' object has no attribute 'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-318-9865d53bacd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtime2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'datetime.date' object has no attribute 'date'"
     ]
    }
   ],
   "source": [
    "time2 = datetimetime1 + diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2010, 10, 2)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(31)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
